
# ðŸ§¼ AutoDQ â€“ Automated Data Quality Monitoring Pipeline

AutoDQ is a plug-and-play, modular data quality monitoring pipeline that automatically ingests datasets dropped into a folder, performs essential data validation checks, logs results, and triggers real-time alerts when issues are found.

Whether you're a data analyst, data engineer, or business analyst, AutoDQ ensures your data is clean, reliable, and production-ready before it flows into dashboards or machine learning models.

---

## ðŸ“Œ Key Features

- âœ… Drop-and-check workflow â€” just drop a dataset into the folder, and AutoDQ does the rest
- ðŸ“‚ File-based ingestion (CSV/XLSX)
- ðŸ“Š Validation checks: missing values, schema mismatches, duplicates (with more rules planned)
- ðŸ§¾ Structured logging for transparency
- ðŸ“£ Real-time alerts via Slack or email
- ðŸ³ Dockerized for consistent execution environments
- â° Optional orchestration using Airflow (Phase 6+)

---

## ðŸ› ï¸ Project Structure

```
AutoDQ/
â”œâ”€â”€ datasets/           # Drop raw input files here
â”œâ”€â”€ configs/            # JSON configs for validation rules (optional)
â”œâ”€â”€ secrets/            # .env file for API keys, DB passwords (not committed)
â”œâ”€â”€ logs/               # Auto-generated log files
â”œâ”€â”€ output/             # (Optional) Cleaned or validated outputs
â”œâ”€â”€ pipeline/           # Core logic scripts (ingestion, validation, alerting)
â”œâ”€â”€ docker/             # Dockerfile and dependencies
â”œâ”€â”€ airflow/            # (Optional) Airflow DAGs
â”œâ”€â”€ run_pipeline.sh     # One-click runner script
â”œâ”€â”€ docker-compose.yml  # For multi-container orchestration
â””â”€â”€ README.md           # This file
```

---

## ðŸ§± Project Phases

| Phase | Description | Status |
|-------|-------------|--------|
| **Phase 1** | File-based ingestion (CSV/XLSX) from `datasets/` folder | âœ… Completed |
| **Phase 2** | Basic data validation (missing values, duplicates) | âœ… Completed |
| **Phase 3** | Logging results and issues | ðŸ”œ Upcoming|
| **Phase 4** | Alert system via Slack or Email | ðŸ”œ |
| **Phase 5** | Dockerize entire pipeline with `.env` support | ðŸ”œ |
| **Phase 6** | Optional scheduling with Airflow or Cron | ðŸ”œ |
| **Phase 7** | Productionization and CI/CD integration | ðŸ”œ |

---

## ðŸ³ Docker Usage

### Build the Docker image

```bash
docker build -f docker/Dockerfile -t autodq .
```

### Run the container

```bash
docker run --rm autodq
```

---

## ðŸ”’ Environment Variables

Add your secrets and credentials to `secrets/.env`:

```
DB_HOST=localhost
DB_USER=user
DB_PASSWORD=password
SLACK_WEBHOOK=https://hooks.slack.com/services/your/webhook/url
```

> Make sure `secrets/.env` is included in `.gitignore`.

---

## ðŸ“Œ License

This project is open-source and available for personal and professional use.
